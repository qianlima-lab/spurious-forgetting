### Code for "Spurious Forgetting in Continual Learning of Language Models"

1. `code_for_biography_dataset`: Experiments on the synthetic Biography dataset 
   - Pretraining on 100K individuals
   - Continual Finetuning on 20K individuals
   - Recovery Experiments
2. `code_for_realworld_scenarios`: Experiments on the realworld scenarios
    - Biography Dataset (EWC, LAMOL, Task Vector, Gradient Projection, SEQ, REPLAY, Freeze)
    - Safety Alignment (Freeze, SEQ)
    - Continual Instruction Tuning (Freeze, SEQ)
    - Continual Knowledge Editing (Freeze, SEQ)
    - Instance Incremental Learning (Freeze, SEQ)