{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pandas as pd \n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_dir = '/dev_data/zjh/LLM_CL/experiments/recovered_models/step19_minus_initialization'\n",
    "\n",
    "# alpha = 0.75\n",
    "step_id = 19\n",
    "\n",
    "backbone = 'EleutherAI/pythia-160m-deduped'\n",
    "path_pretrained = f'/dev_data/zjh/physics-of-forgetting-in-llm/physics_of_forgetting/model/gpt-neox/v_0720/multi5_permute_fullname/full_fine_tuning/lr_5e-06_wd_0.01/final_model'  # model pre-trained on 100k biography data, fine-tuned on task0\n",
    "# path_step = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_epoch_1_bs_240_epoch_25_lr5e-5/2024-08-07-22-07-33/checkpoint_llm_task_0_epoch_0_step_{step_id}'  # model fine-tuned on task1 (epoch_1 lr5e-5)\n",
    "for e in [20, 22, 24]:\n",
    "    path_step = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_bs_240_epoch_25_lr5e-5/2024-08-07-12-52-52/checkpoint_llm_task_0_epoch_{e}'\n",
    "    save_dir = f'/dev_data/zjh/LLM_CL/experiments/recovered_models/epoch{e}_minus_initialization'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(backbone)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(path_pretrained)\n",
    "    model_pretrained = AutoModelForCausalLM.from_pretrained(path_pretrained, config=config)  # get the pre-trained model\n",
    "\n",
    "    config = AutoConfig.from_pretrained(path_step)\n",
    "    model_step = AutoModelForCausalLM.from_pretrained(path_step, config=config)  # ge the model that is fine-tuned on task1 for several steps (currently 19 steps)\n",
    "\n",
    "    for alpha in [0.16, 0.18, 0.2, 0.22, 0.24, 0.4, 0.6, 0.8, 1.0]:\n",
    "        for epoch_id in range(25):\n",
    "\n",
    "            path_epoch = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_bs_240_epoch_25_lr5e-5/2024-08-07-12-52-52/checkpoint_llm_task_0_epoch_{epoch_id}'  # model fine-tuned on task1 (lr5e-5)\n",
    "\n",
    "            config = AutoConfig.from_pretrained(path_epoch)\n",
    "            model_epoch = AutoModelForCausalLM.from_pretrained(path_epoch, config=config)\n",
    "\n",
    "            save_ckpt_name = f'model_epoch{epoch_id}_add_epoch{e}_minus_pretrained_alpha{alpha}'\n",
    "            for (n1,p1), (n2,p2), (n3,p3) in zip(model_pretrained.named_parameters(), \n",
    "                                                model_step.named_parameters(), \n",
    "                                                model_epoch.named_parameters()):\n",
    "                assert n1==n2 and n2==n3\n",
    "                p3.data = p3.data-alpha*(p2.data-p1.data)\n",
    "            save_path = os.path.join(save_dir, save_ckpt_name)\n",
    "            if os.path.exists(save_path):\n",
    "                raise ValueError(f\"{save_ckpt_name} already exists.\")\n",
    "            model_epoch.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_of_task_vector in [20, 22, 24]:\n",
    "    task_vector_start_dir = f'/dev_data/zjh/physics-of-forgetting-in-llm/physics_of_forgetting/model/gpt-neox/v_0720/multi5_permute_fullname/full_fine_tuning/lr_5e-06_wd_0.01/final_model'\n",
    "    config = AutoConfig.from_pretrained(task_vector_start_dir)\n",
    "    task_vector_start_model = AutoModelForCausalLM.from_pretrained(task_vector_start_dir, config=config)\n",
    "\n",
    "    task_vector_end_dir = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_bs_240_epoch_25_lr5e-5/2024-08-07-12-52-52/checkpoint_llm_task_0_epoch_{epoch_of_task_vector}'\n",
    "    config = AutoConfig.from_pretrained(task_vector_end_dir)\n",
    "    task_vector_end_model = AutoModelForCausalLM.from_pretrained(task_vector_end_dir, config=config)\n",
    "    for epoch_ckpt in range(0, 25, 2):\n",
    "        for alpha in [0.16, 0.18, 0.2, 0.22, 0.24, 0.4, 0.6, 0.8, 1.0]:\n",
    "            model_to_be_modified_dir = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_bs_240_epoch_25_lr5e-5/2024-08-07-12-52-52/checkpoint_llm_task_0_epoch_{epoch_ckpt}'\n",
    "            config = AutoConfig.from_pretrained(model_to_be_modified_dir)\n",
    "            model_to_be_modified = AutoModelForCausalLM.from_pretrained(model_to_be_modified_dir, config=config)\n",
    "\n",
    "            for (n_start, p_start), (n_end, p_end), (n, p) in zip(task_vector_start_model.named_parameters(), task_vector_end_model.named_parameters(), model_to_be_modified.named_parameters()):\n",
    "                assert n_start==n_end and n_end==n\n",
    "                p.data = p.data - alpha * (p_end.data - p_start.data)\n",
    "            save_dir = f'/dev_data/zjh/LLM_CL/experiments/recovered_models/epoch{epoch_of_task_vector}_minus_initialization'\n",
    "            save_ckpt_name = f'model_epoch{epoch_ckpt}_add_epoch{epoch_of_task_vector}_minus_pretrained_alpha{alpha}'\n",
    "            save_path = os.path.join(save_dir, save_ckpt_name)\n",
    "            if os.path.exists(save_path):\n",
    "                raise ValueError(f\"{save_ckpt_name} already exists.\")\n",
    "            model_to_be_modified.save_pretrained(save_path)\n",
    "\n",
    "for epoch_of_task_vector in [20, 22, 24]:\n",
    "    # task_vector_start_dir = f'/dev_data/zjh/physics-of-forgetting-in-llm/physics_of_forgetting/model/gpt-neox/v_0720/multi5_permute_fullname/full_fine_tuning/lr_5e-06_wd_0.01/final_model'\n",
    "    task_vector_start_dir = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_epoch_1_bs_240_epoch_25_lr5e-5/2024-08-07-22-07-33/checkpoint_llm_task_0_epoch_0_step_19'\n",
    "    config = AutoConfig.from_pretrained(task_vector_start_dir)\n",
    "    task_vector_start_model = AutoModelForCausalLM.from_pretrained(task_vector_start_dir, config=config)\n",
    "\n",
    "    task_vector_end_dir = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_bs_240_epoch_25_lr5e-5/2024-08-07-12-52-52/checkpoint_llm_task_0_epoch_{epoch_of_task_vector}'\n",
    "    config = AutoConfig.from_pretrained(task_vector_end_dir)\n",
    "    task_vector_end_model = AutoModelForCausalLM.from_pretrained(task_vector_end_dir, config=config)\n",
    "    for epoch_ckpt in range(0, 25, 2):\n",
    "        for alpha in [0.16, 0.18, 0.2, 0.22, 0.24, 0.4, 0.6, 0.8, 1.0]:\n",
    "            model_to_be_modified_dir = f'/dev_data/zjh/LLM_CL/experiments/Outdomain-Finetuned-tmp_SEQ_bs_240_epoch_25_lr5e-5/2024-08-07-12-52-52/checkpoint_llm_task_0_epoch_{epoch_ckpt}'\n",
    "            config = AutoConfig.from_pretrained(model_to_be_modified_dir)\n",
    "            model_to_be_modified = AutoModelForCausalLM.from_pretrained(model_to_be_modified_dir, config=config)\n",
    "\n",
    "            for (n_start, p_start), (n_end, p_end), (n, p) in zip(task_vector_start_model.named_parameters(), task_vector_end_model.named_parameters(), model_to_be_modified.named_parameters()):\n",
    "                assert n_start==n_end and n_end==n\n",
    "                p.data = p.data - alpha * (p_end.data - p_start.data)\n",
    "            save_dir = f'/dev_data/zjh/LLM_CL/experiments/recovered_models/epoch{epoch_of_task_vector}_minus_step19'\n",
    "            save_ckpt_name = f'model_epoch{epoch_ckpt}_add_epoch{epoch_of_task_vector}_minus_step19_alpha{alpha}'\n",
    "            save_path = os.path.join(save_dir, save_ckpt_name)\n",
    "            if os.path.exists(save_path):\n",
    "                raise ValueError(f\"{save_ckpt_name} already exists.\")\n",
    "            model_to_be_modified.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect Recovered-Evaluate-IND-Tradeoff\n",
    "# /dev_data/zjh/LLM_CL/experiments/Recovered-Evaluate-IND-Tradeoff-epoch24-add-epoch18-alpha1.0-SEQ_epoch_1\n",
    "all_column_list = []\n",
    "all_acc_list = []\n",
    "for epoch_of_task_vector in [12, 14, 16, 18]:\n",
    "    for alpha in [0.2, 1.0]:\n",
    "        # print(f'epoch_of_task_vector: {epoch_of_task_vector}, alpha: {alpha}')\n",
    "        all_column_list.append(f'epoch_of_task_vector_{epoch_of_task_vector}_alpha_{alpha}')\n",
    "        task_0_acc_list = []\n",
    "        for epoch_ckpt in range(0, 26, 2):\n",
    "            path = f'/dev_data/zjh/LLM_CL/experiments/Recovered-Evaluate-IND-Tradeoff-epoch{epoch_ckpt}-add-epoch{epoch_of_task_vector}-alpha{alpha}-SEQ_epoch_1'\n",
    "            all_dir = os.listdir(path)\n",
    "            path = os.path.join(path, all_dir[0])\n",
    "            matrix_path = os.path.join(path, 'test_cur_task_0_save_result.npy')\n",
    "            train_log_path = os.path.join(path, 'train.log')\n",
    "            matrix = np.load(matrix_path, allow_pickle=True)  # get [None] ðŸ˜¢\n",
    "            with open(train_log_path, 'r') as f:\n",
    "                train_log = f.readlines()\n",
    "                for line in train_log:\n",
    "                    # use regular expression to search for pattern: Test Result = {'Test_Acc_Task_0': 99.626, 'Test_Acc_Task_Seen': 99.626, 'Test_Acc_Task_All': 99.626}\n",
    "                    match = re.search(r\"Test Result = \\{'Test_Acc_Task_0': (\\d+\\.\\d+), 'Test_Acc_Task_Seen': (\\d+\\.\\d+), 'Test_Acc_Task_All': (\\d+\\.\\d+)\\}\", line)\n",
    "                    if match:\n",
    "                        task_0_acc = match.group(1)\n",
    "                        task_0_acc_list.append(float(task_0_acc))\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Test Result not found in {train_log_path}\")\n",
    "        all_acc_list.append(task_0_acc_list)\n",
    "print(all_acc_list)\n",
    "all_acc_arr = np.array(all_acc_list).T\n",
    "result_df = pd.DataFrame(all_acc_arr, columns=all_column_list)\n",
    "print(result_df)\n",
    "result_df.to_csv('/dev_data/zjh/LLM_CL/visualization-tradeoff/temp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect Recovered-Evaluate-Tradeoff-Sample\n",
    "# /dev_data/zjh/LLM_CL/experiments/Recovered-Evaluate-Tradeoff-Sample-epoch24-add-epoch18-alpha1.0-tmp_Evaluate_bs_1440_sample_2000\n",
    "task0_column_list = []\n",
    "task0_acc_list = []\n",
    "task1_column_list = []\n",
    "task1_acc_list = []\n",
    "for epoch_of_task_vector in [12, 14, 16, 18]:\n",
    "    for alpha in [0.16, 0.18, 0.2, 0.22, 0.24, 0.4, 0.6, 0.8, 1.0]:\n",
    "        task0_column_list.append(f'epoch_of_task_vector_{epoch_of_task_vector}_alpha_{alpha}')\n",
    "        task1_column_list.append(f'epoch_of_task_vector_{epoch_of_task_vector}_alpha_{alpha}')\n",
    "        _task0_acc_list = []\n",
    "        _task1_acc_list = []\n",
    "        for epoch_ckpt in range(0, 26, 2):\n",
    "            path = f'/dev_data/zjh/LLM_CL/experiments/Recovered-Evaluate-Tradeoff-Sample-epoch{epoch_ckpt}-add-epoch{epoch_of_task_vector}-alpha{alpha}-tmp_Evaluate_bs_1440_sample_2000'\n",
    "            all_dir = os.listdir(path) \n",
    "            path = os.path.join(path, all_dir[0])\n",
    "            train_log_path = os.path.join(path, 'train.log')\n",
    "            with open(train_log_path, 'r') as f:\n",
    "                train_log = f.readlines()\n",
    "                for line in train_log:\n",
    "                    # find Test Result = {'Test_Acc_Task_0': 99.775, 'Test_Acc_Task_1': 0.875, 'Test_Acc_Task_2': 0.0, 'Test_Acc_Task_3': 16.667, 'Test_Acc_Task_4': 0.0, 'Test_Acc_Task_5': 0.0, 'Test_Acc_Task_Seen': 19.553, 'Test_Acc_Task_All': 19.553}\n",
    "                    match = re.search(r\"Test Result = \\{'Test_Acc_Task_0': (\\d+\\.\\d+), 'Test_Acc_Task_1': (\\d+\\.\\d+), 'Test_Acc_Task_2': (\\d+\\.\\d+), 'Test_Acc_Task_3': (\\d+\\.\\d+), 'Test_Acc_Task_4': (\\d+\\.\\d+), 'Test_Acc_Task_5': (\\d+\\.\\d+), 'Test_Acc_Task_Seen': (\\d+\\.\\d+), 'Test_Acc_Task_All': (\\d+\\.\\d+)\\}\", line)\n",
    "                    if match:\n",
    "                        task0_acc, task1_acc = match.group(1), match.group(2)\n",
    "                        _task0_acc_list.append(float(task0_acc))\n",
    "                        _task1_acc_list.append(float(task1_acc))\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Test Result not found in {train_log_path}\")\n",
    "        task0_acc_list.append(_task0_acc_list)\n",
    "        task1_acc_list.append(_task1_acc_list)\n",
    "print(task0_acc_list)\n",
    "print(task1_acc_list)\n",
    "task0_acc_arr = np.array(task0_acc_list).T\n",
    "task1_acc_arr = np.array(task1_acc_list).T\n",
    "task0_result_df = pd.DataFrame(task0_acc_arr, columns=task0_column_list)\n",
    "task1_result_df = pd.DataFrame(task1_acc_arr, columns=task1_column_list)\n",
    "print(task0_result_df)\n",
    "print(task1_result_df)\n",
    "task0_result_df.to_csv('/dev_data/zjh/LLM_CL/visualization-tradeoff/task0_temp.csv')\n",
    "task1_result_df.to_csv('/dev_data/zjh/LLM_CL/visualization-tradeoff/task1_temp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
